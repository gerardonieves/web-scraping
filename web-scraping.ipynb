{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Web scraping is a task related to gathering data from the web; this might include text, numbers, images, videos, etc...  \n",
    "The process of web scraping is simple:\n",
    "1. download the web page\n",
    "2. extract stuff from the web page\n",
    "\n",
    "Yes, the process is simple but web pages are messy. Behind the hood of those pretty web sites we look at is a lot of structuring that's put in place by markup languages like HTML and CSS. This messiness, like in the image below, is typically hard to parse for someone who isn't familiar with building websites.\n",
    "\n",
    "<img src=\"raw-html.png\" alt=\"raw html\" style=\"width: 70%; height: 70%\"/>\n",
    "\n",
    "But that's why we have Python packages :).\n",
    "\n",
    "Web scraping is used a lot in the real world for many purposes: to store data in databases for later analysis, to make on-the-fly decisions (like buying stocks), or creating a bot that retrieves the news and tries to summarize it for you in a paragraph or less. I (Bobby) have personally used web scraping to help small to medium sized businesses collect more data, make more data-driven decisions, and to automate a lot of mundane processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install: ```pip install beautifulsoup4```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The [requests](http://docs.python-requests.org/en/master/) package is used for easily making web calls to download web pages and whatnot; it's for humans.\n",
    "* ```re``` is for regular expressions\n",
    "* ```pprint``` \"pretty-prints\" an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the HTML from a Web Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'https://twitter.com/realDonaldTrump'\n",
    "web_page_html = requests.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(web_page_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the Title of the Web Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Donald J. Trump (@realDonaldTrump) | Twitter</title>\n",
      "Donald J. Trump (@realDonaldTrump) | Twitter\n"
     ]
    }
   ],
   "source": [
    "print(soup.title)             # Prints the web page title including the tags\n",
    "print(soup.title.text)        # Prints just the title text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We Can Also Print the Body of the Web Page (i.e. no headers, footers, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(soup.body)             # Prints the body of the web page (including all tags)\n",
    "print(soup.body.text)        # Prints the text in the body stripped of all tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing Tagged Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HTML, a link is created by using the tag ```<a>``` with the property ```href```. To find all links with this property, we can use the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/help/verified',\n",
       " '/realDonaldTrump/likes',\n",
       " '/realDonaldTrump',\n",
       " '/help/verified',\n",
       " '/realDonaldTrump/media',\n",
       " '/realDonaldTrump/media',\n",
       " '/realDonaldTrump/with_replies',\n",
       " '/realDonaldTrump/media',\n",
       " 'https://t.co/I4Vz1mRdBK',\n",
       " 'https://t.co/o7YNUNwb8f',\n",
       " 'https://t.co/48iZam5Fai',\n",
       " 'https://t.co/okiZ8ZeDgz',\n",
       " '/search?q=place%3A07d9dafff4481002',\n",
       " '/realDonaldTrump/status/824083821889015809',\n",
       " '/realDonaldTrump/status/824080766288228352']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a few links\n",
    "links = [link['href'] for link in soup.findAll('a', href=True, text=re.compile(r''))]\n",
    "links[45:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking for links with \"Trump\" in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/realDonaldTrump\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/following\n",
      "/realDonaldTrump/followers\n",
      "/realDonaldTrump/likes\n",
      "/realDonaldTrump/likes\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/media\n",
      "/realDonaldTrump/media\n",
      "/realDonaldTrump/with_replies\n",
      "/realDonaldTrump/media\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/824448880993509376\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/824448156935081984\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/824440456813707265\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/824407390674157568\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/824377804590563339\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/824229586091307008\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/824228768227217408\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/824227824903090176\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/824083821889015809\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/824080766288228352\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/824078417213747200\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/824055927200423936\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/823950814163140609\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/823939422743830528\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/823937936056008704\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/823850781946343427\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/823495059010109440\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/823174199036542980\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/823151124815507460\n",
      "/realDonaldTrump\n",
      "/realDonaldTrump/status/823150055418920960\n"
     ]
    }
   ],
   "source": [
    "for link in soup.findAll('a', href=True):\n",
    "    if 'Trump' in link['href']:\n",
    "        print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Text by Drilling into Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"trump-twitter-descr.png\" alt=\"trump description in twitter\" style:\"width:70%, height:70%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45th President of the United States of America\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('div',{\"class\":\"ProfileHeaderCard\"}).find('p').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a similar query to gather all the tweets on the page by drilling into all ```p``` tags that have the class ```TweetTextSize TweetTextSize--26px js-tweet-text tweet-text```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \"@romoabcnews: .@DavidMuir first @POTUS interview since taking office.  Tonight on @ABCWorldNews @ABC2020 tonight. pic.twitter.com/I4Vz1mRdBK\" \n",
      "\n",
      "1 As your President, I have no higher duty than to protect the lives of the American people.pic.twitter.com/o7YNUNwb8f \n",
      "\n",
      "2 I will be interviewed by @DavidMuir tonight at 10 o'clock on @ABC. Will be my first interview from the White House. Enjoy!pic.twitter.com/okiZ8ZeDgz â€“ at The White House \n",
      "\n",
      "3 Big day planned on NATIONAL SECURITY tomorrow. Among many other things, we will build the wall! \n",
      "\n",
      "4 If Chicago doesn't fix the horrible \"carnage\" going on, 228 shootings in 2017 with 42 killings (up 24% from 2016), I will send in the Feds! \n",
      "\n",
      "5 Signing orders to move forward with the construction of the Keystone XL and Dakota Access pipelines in the Oval Office.pic.twitter.com/OErGmbBvYK â€“ at The Oval Office \n",
      "\n",
      "6 Peaceful protests are a hallmark of our democracy. Even if I don't always agree, I recognize the rights of people to express their views. \n",
      "\n",
      "7 Wow, television ratings just out: 31 million people watched the Inauguration, 11 million more than the very good ratings from 4 years ago! \n",
      "\n",
      "8 Watched protests yesterday but was under the impression that we just had an election! Why didn't these people vote? Celebs hurt cause badly. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = [tweets.text for tweets in soup.findAll('p',{\"class\":\"TweetTextSize TweetTextSize--26px js-tweet-text tweet-text\"})]\n",
    "for index, tweet in enumerate(tweets):\n",
    "    print(index, tweet, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling the Web Browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of websites today use JavaScript to control the behavior of their sites (like Single Page Applications). Because of this, if one were to write a program to navigate and scrape data throughout a website, making simple URL calls is not enough - you need to control a web browser.\n",
    "\n",
    "The [selenium API for Python](https://selenium-python.readthedocs.io/getting-started.html#simple-usage) allows you to control a web browser like Chrome, Firefox, or Safari.\n",
    "* Provides easy way to handle JavaScript when trying to scrape data from a web page\n",
    "* Great way to automate web testing\n",
    "\n",
    "To install\n",
    "* ```pip install selenium```\n",
    "* ```brew install chromedriver``` (for Mac OS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.execute_script('window.focus()')\n",
    "driver.get(\"http://www.python.org\")\n",
    "assert \"Python\" in driver.title\n",
    "elem = driver.find_element_by_name(\"q\")\n",
    "elem.clear()\n",
    "elem.send_keys(\"pycon\")\n",
    "elem.send_keys(Keys.RETURN)\n",
    "assert \"No results found.\" not in driver.page_source\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newspaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Newspaper](https://newspaper.readthedocs.io/en/latest/) is a Python library specifically for extracting and curating articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On most web sites, there's typically a file in the root of their web site called ```robots.txt``` ([check out Twitter's](https://twitter.com/robots.txt)). This plain-text file contains directories that the web site doesn't want web bots (like the ones used by Google to index the internet) to access. Althought there's not a law that says you can't access those \"forbidden\" locations, it's typically a courtesy not to.\n",
    "\n",
    "Also, one must take into account how many times they are requesting a web page from a web site. If you do it too often, some websites might automtically ban your IP from accessing the site. Or worse, you send their site so many requests that their servers can't handle the load and their web site goes down - this is very much like a DoS (Denial of Service) attack and you could get sued or go to jail, depending on the damage and your intentions.\n",
    "\n",
    "So in short, when scraping a web page, don't do this:\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    scrape_page\n",
    "```\n",
    "\n",
    "Be smart about how often you scrape data from a site and if you're really clever, you'd collect user-behavior data from a website and use that to train your web scraper to request pages as often as a human would."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scrape SpaceX future missions table from [this](http://www.spacex.com/missions) website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           customers           launch_sites  \\\n",
      "0               Arabsat (Arabsat 6A)         Cape Canaveral   \n",
      "1                      Bangabandhu-1         Cape Canaveral   \n",
      "2                  Bigelow Aerospace         Cape Canaveral   \n",
      "3                       Bulgaria Sat         Cape Canaveral   \n",
      "4                  CONAE (Argentina)             Vandenberg   \n",
      "5                  CONAE (Argentina)             Vandenberg   \n",
      "6                DragonLab Mission 1         Cape Canaveral   \n",
      "7                DragonLab Mission 2         Cape Canaveral   \n",
      "8                         Es'hailsat         Cape Canaveral   \n",
      "9                  Falcon Heavy Demo         Cape Canaveral   \n",
      "10                          HISPASAT         Cape Canaveral   \n",
      "11                          Inmarsat         Cape Canaveral   \n",
      "12                          Inmarsat         Cape Canaveral   \n",
      "13                          Intelsat         Cape Canaveral   \n",
      "14                Iridium (Flight 2)             Vandenberg   \n",
      "15                Iridium (Flight 3)             Vandenberg   \n",
      "16                Iridium (Flight 4)             Vandenberg   \n",
      "17                Iridium (Flight 5)             Vandenberg   \n",
      "18                Iridium (Flight 6)             Vandenberg   \n",
      "19                Iridium (Flight 7)             Vandenberg   \n",
      "20                          Koreasat         Cape Canaveral   \n",
      "21                       NASA (TESS)         Cape Canaveral   \n",
      "22                NASA Crew (Demo 1)  Florida (Launch Site)   \n",
      "23                NASA Crew (Demo 2)  Florida (Launch Site)   \n",
      "24  NASA Resupply to ISS (Flight 10)         Cape Canaveral   \n",
      "25  NASA Resupply to ISS (Flight 11)         Cape Canaveral   \n",
      "26  NASA Resupply to ISS (Flight 12)         Cape Canaveral   \n",
      "27  NASA Resupply to ISS (Flight 13)         Cape Canaveral   \n",
      "28  NASA Resupply to ISS (Flight 14)         Cape Canaveral   \n",
      "29  NASA Resupply to ISS (Flight 15)         Cape Canaveral   \n",
      "30                  Northrop Grumman         Cape Canaveral   \n",
      "31                     NSPO (Taiwan)             Vandenberg   \n",
      "32                     OHB System AG             Vandenberg   \n",
      "33                          Radarsat             Vandenberg   \n",
      "34                      SES (SES-10)         Cape Canaveral   \n",
      "35                      SES (SES-11)         Cape Canaveral   \n",
      "36                      SES (SES-14)         Cape Canaveral   \n",
      "37                      SES (SES-16)         Cape Canaveral   \n",
      "38                          Spacecom         Cape Canaveral   \n",
      "39              US Air Force (STP-2)         Cape Canaveral   \n",
      "40                            ViaSat         Cape Canaveral   \n",
      "\n",
      "              vehicles  \n",
      "0         Falcon Heavy  \n",
      "1             Falcon 9  \n",
      "2             Falcon 9  \n",
      "3             Falcon 9  \n",
      "4             Falcon 9  \n",
      "5             Falcon 9  \n",
      "6   Dragon  & Falcon 9  \n",
      "7   Dragon  & Falcon 9  \n",
      "8             Falcon 9  \n",
      "9         Falcon Heavy  \n",
      "10            Falcon 9  \n",
      "11        Falcon Heavy  \n",
      "12            Falcon 9  \n",
      "13        Falcon Heavy  \n",
      "14            Falcon 9  \n",
      "15            Falcon 9  \n",
      "16            Falcon 9  \n",
      "17            Falcon 9  \n",
      "18            Falcon 9  \n",
      "19            Falcon 9  \n",
      "20            Falcon 9  \n",
      "21            Falcon 9  \n",
      "22  Dragon  & Falcon 9  \n",
      "23  Dragon  & Falcon 9  \n",
      "24  Dragon  & Falcon 9  \n",
      "25  Dragon  & Falcon 9  \n",
      "26  Dragon  & Falcon 9  \n",
      "27  Dragon  & Falcon 9  \n",
      "28  Dragon  & Falcon 9  \n",
      "29  Dragon  & Falcon 9  \n",
      "30            Falcon 9  \n",
      "31            Falcon 9  \n",
      "32            Falcon 9  \n",
      "33            Falcon 9  \n",
      "34            Falcon 9  \n",
      "35            Falcon 9  \n",
      "36            Falcon 9  \n",
      "37            Falcon 9  \n",
      "38            Falcon 9  \n",
      "39        Falcon Heavy  \n",
      "40        Falcon Heavy  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'http://www.spacex.com/missions'\n",
    "web_page_html = requests.get(url).text\n",
    "soup = BeautifulSoup(web_page_html, \"html.parser\")\n",
    "# there are 2 tables, get the 2nd one\n",
    "future_missions_table = soup.findAll('table', {\"class\":\"views-table cols-4\"})[1]\n",
    "customers, launch_sites, vehicles = [], [], []\n",
    "for row in future_missions_table.findAll('tr'):\n",
    "    customers.append(row.find('td', {\"class\":\"customer\"}).find('span').text.strip())\n",
    "    launch_sites.append(row.find('td', {\"class\":\"launch-site\"}).find('span').text.strip())\n",
    "    vehicles.append(row.find('td', {\"class\":\"vehicle\"}).find('span').text.strip().replace('\\n', ''))\n",
    "\n",
    "# pprint(customers)\n",
    "# pprint(launch_sites)\n",
    "# pprint(vehicles)\n",
    "future_missions_df = pd.DataFrame(\n",
    "    {'customers': customers,\n",
    "     'launch_sites': launch_sites,\n",
    "     'vehicles': vehicles\n",
    "    })\n",
    "print(future_missions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
